
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>FER Ansätze &#8212; Facial Emotion Recognition</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Implementation" href="fem_impl.html" />
    <link rel="prev" title="Daten(sätze)" href="datasets.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Facial Emotion Recognition</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="index.html">
   Facial Emotion Recognition
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="definition.html">
   Was ist Facial Emotion Recognition?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="human_communication.html">
   Menschliche Kommunikation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="featureextraction.html">
   Mimikcodierung und Feature Extraction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="datasets.html">
   Daten(sätze)
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   FER Ansätze
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="fem_impl.html">
   Implementation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="VideoTest.html">
   Selbsttrainiertes Modell per Webcam testen
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="criticism.html">
   Kritik
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="conclusion.html">
   Fazit
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="references.html">
   Quellen
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/models.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/jgrimm22/facial-emotion-recognition"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/jgrimm22/facial-emotion-recognition/issues/new?title=Issue%20on%20page%20%2Fmodels.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#konventioneller-ansatz">
   Konventioneller Ansatz
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#image-preprocessing">
     1. Image Preprocessing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#feature-extraction">
     2.	Feature Extraction
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expression-classification">
     3. Expression Classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deep-learning-basierte-ansatze">
   Deep-Learning-basierte Ansätze
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convolutional-neural-network-cnn">
     Convolutional Neural Network (CNN)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#long-short-term-memory-lstm">
     Long Short-Term Memory (LSTM)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#weitere-modelle">
     Weitere Modelle
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="fer-ansatze">
<h1>FER Ansätze<a class="headerlink" href="#fer-ansatze" title="Permalink to this headline">¶</a></h1>
<div class="section" id="konventioneller-ansatz">
<h2>Konventioneller Ansatz<a class="headerlink" href="#konventioneller-ansatz" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>Image Preprocessing</p></li>
<li><p>Feature Extraction</p></li>
<li><p>Expression Classification</p></li>
</ol>
<ul class="simple">
<li><p>Geringere Rechenleistung und Speicherbedarf als Deep-Learning-basierte Ansätze</p></li>
<li><p>Weniger abhängig von Daten und Hardware</p></li>
<li><p>Merkmalsextraktion und die Klassifizierung müssen manuell und separat entwickelt werden, was bedeutet, dass diese beiden Phasen nicht gleichzeitig optimiert werden können</p></li>
</ul>
<p>Konventionelle FER-Verfahren kann in drei Hauptschritte unterteilt werden:</p>
<p><img alt="Konventioneller FER-Ansatz" src="_images/conventional_model.png" /></p>
<div class="section" id="image-preprocessing">
<h3>1. Image Preprocessing<a class="headerlink" href="#image-preprocessing" title="Permalink to this headline">¶</a></h3>
<p>Das Ziel des Image Preprocessing ist die Eliminierung irrelevanter Informationen aus den Eingabebildern und die Verbesserung der Erkennungsfähigkeit relevanter Informationen.</p>
<ul class="simple">
<li><p>Kann sich direkt auf die Extraktion von Merkmalen und die Leistung der Expression Classification auswirken</p></li>
<li><p>Bilder können aus verschieden Gründen durch andere Signale „verunreinigt“ sein (komplexe Hintergründe, Lichtintensität, Verdeckung etc.)</p></li>
</ul>
<p>Image Preprocessing Prozesse sind:</p>
<ul class="simple">
<li><p>Noise Reduction: Average Filter (AF), Gaussian Filter (GF ), Median Filter (MF), Bilateral Filter (BF)</p></li>
<li><p>Face Detection: Gesichtserkennung hat sich zu einem eigenständigen Gebiet entwickelt, Vorstufe in FER mit dem Ziel, die Gesichtsregion zu lokalisiern und zu extrahieren</p></li>
<li><p>Normalisierung der Skala und der Graustufen: Normalisierung von Größe und Farbe der Eingabebilder, mit dem Ziel Berechnungskomplexität zu reduzieren unter der Prämisse, die wichtigsten Merkmale des Gesichts zu erhalten</p></li>
<li><p>Histogramm-Entzerrung: Verbesserung der Bildwirkung</p></li>
</ul>
</div>
<div class="section" id="feature-extraction">
<h3>2.	Feature Extraction<a class="headerlink" href="#feature-extraction" title="Permalink to this headline">¶</a></h3>
<p>Die Feature Extraction ist der Prozess zur Extraktion nützlicher Daten oder Informationen aus dem Bild, z.B. Werte, Vektoren und Symbole. Dieser Schritt ist von Bedeutung, da sich die Feature Extraction direkt auf die Leistung der Algorithmen auswirken kann.</p>
</div>
<div class="section" id="expression-classification">
<h3>3. Expression Classification<a class="headerlink" href="#expression-classification" title="Permalink to this headline">¶</a></h3>
<p>Der Gesichtsausdruck wird basierend auf eine der Gesichtskategorien bestimmt. Die Kategorien werden mit Hilfe von Pattern Classifiers vortrainiert.</p>
<ul class="simple">
<li><p>Weitverbreite Classifier:</p>
<ul>
<li><p>kNN (k-Nearest Neighbours )</p></li>
<li><p>SVM (Support Vector Machine)</p></li>
<li><p>Adaboost (Adaptive Boosting)</p></li>
<li><p>Bayessches Netz</p></li>
<li><p>SRC (Sparse Representation-based Classifier)</p></li>
<li><p>PNN (Probabilistic Neural Network)</p></li>
</ul>
</li>
</ul>
<!-- #region -->
</div>
</div>
<div class="section" id="deep-learning-basierte-ansatze">
<h2>Deep-Learning-basierte Ansätze<a class="headerlink" href="#deep-learning-basierte-ansatze" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Deep-Learning-basierte Algorithmen für die Merkmalsextraktion, Klassifizierung und Erkennungsaufgaben</p></li>
<li><p>Bei vielen Aufgaben des maschinellen Lernens hervorragende Leistungen gezeigt</p></li>
<li><p>Bei FER:</p>
<ul>
<li><p>Reduzierung der Abhängigkeit von der Bildvorverarbeitung und Merkmalsextraktion</p></li>
<li><p>Robust gegenüber Umgebungen mit unterschiedlichen Elementen, z.B. Beleuchtung und Verdeckung</p></li>
</ul>
</li>
<li><p>Fähigkeit, große Datenmengen zu verarbeiten</p></li>
</ul>
<div class="section" id="convolutional-neural-network-cnn">
<h3>Convolutional Neural Network (CNN)<a class="headerlink" href="#convolutional-neural-network-cnn" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>State-of-the-art in FER</p></li>
<li><p>Implementation als “End-to-End”-Modell: Lernen direkt von den Eingabedaten zum Klassifikationsergebnis</p></li>
<li><p>Merkmale:</p>
<ul>
<li><p>Weniger Netzwerkparameter durch lokale Konnektivität und die gemeinsame Nutzung von Gewichten</p></li>
<li><p>Schnelle Trainingsgeschwindigkeit</p></li>
</ul>
</li>
<li><p>CNN enthält drei Arten von heterogenen Schichten:</p>
<ul>
<li><p>Convolutional Layer: Eingabebilder werden mit Hilfe von Filtern gefaltet und es wird eine Feature Map erzeugt</p></li>
<li><p>(Max) Pooling Layer: (Max) Pooling-Layers (Subsampling) senken die räumliche Auflösung der gegebenen Feature Maps</p></li>
<li><p>Fully Connected Layer: berechnen die Klassen-Scores auf dem gesamten Originalbild und ein einzelner Gesichtsausdruck wird basierend auf der Ausgabe von Softmax-Algorithmus erkannt</p></li>
</ul>
</li>
</ul>
<p><img alt="CNN FER-Ansatz" src="_images/CNN_model.png" /></p>
</div>
<div class="section" id="long-short-term-memory-lstm">
<h3>Long Short-Term Memory (LSTM)<a class="headerlink" href="#long-short-term-memory-lstm" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Art von RNN (Recurrent Neural Network), das aus LSTM-Einheiten besteht</p></li>
<li><p>“Langes Kurzeitgedächtnis”: Im Gegensatz zu herkömmlichen RNN, hat das LSTM durch den Einsatz verschiedener Gates eine Art Erinnerung an frühere Erfahrungen</p></li>
<li><p>Lösen das Problem der verschwindenden Gradienten, welches in RNNs vorkommt</p></li>
<li><p>Geeignet für die zeitliche Merkmalsextraktion von aufeinanderfolgenden Frames</p></li>
</ul>
<p>Ein LSTM-Netz hat drei Gates, die die Zellzustände aktualisieren und steuern:</p>
<ol class="simple">
<li><p>Forget-Gate: steuert, welche Informationen im Zellzustand vergessen werden sollen, wenn neue Informationen in das Netzwerk gelangen</p></li>
<li><p>Input-Gate: steuert, welche neuen Informationen in den Zellzustand kodiert werden, wenn die neuen Input-Informationen vorliegen</p></li>
<li><p>Output-Gate: steuert, welche im Zellzustand kodierte Information im folgenden Zeitschritt als Input an das Netzwerk gesendet wird
<img alt="LSTM" src="_images/LSTM.gif" /></p></li>
</ol>
<!-- #endregion -->
</div>
<div class="section" id="weitere-modelle">
<h3>Weitere Modelle<a class="headerlink" href="#weitere-modelle" title="Permalink to this headline">¶</a></h3>
<p>Es gibt viele Ansätze, die auf einem eigenständigen CNN oder einer Kombination aus LSTM und CNN basieren:</p>
<ul class="simple">
<li><p>3D Convolutional Neural Network (3DCNN)</p></li>
<li><p>3D Inception-ResNet</p></li>
<li><p>DRML (Deep Region und Multi-Label-Learning)</p></li>
<li><p>Candide-3</p></li>
<li><p>Multi-Angle FER</p></li>
<li><p>Hybrid CNN-RNN</p></li>
</ul>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="datasets.html" title="previous page">Daten(sätze)</a>
    <a class='right-next' id="next-link" href="fem_impl.html" title="next page">Implementation</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Jessica Hofmann, Julia Grimm<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>